{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 85.0.4183\n",
      "[WDM] - Get LATEST driver version for 85.0.4183\n",
      "[WDM] - Get LATEST driver version for 85.0.4183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/85.0.4183.87/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\tariq\\.wdm\\drivers\\chromedriver\\win32\\85.0.4183.87]\n"
     ]
    }
   ],
   "source": [
    "#Importing necessery libraries Selenium to simulate a browser, beautiful soup to parse HTMl and pandas to export to csv\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "#initializing selenium\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsArray=[]\n",
    "storiesArray=[]\n",
    "urls=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a url scrapes the page and turns it to BS4 object\n",
    "def toSoup(url):\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return soup\n",
    "#Turns an array of paragraphs to a single string\n",
    "def paragraphArrayToSingleString(paragraphs):\n",
    "    string=\"\"\n",
    "    for paragraph in paragraphs:\n",
    "        string=string+paragraph.text.strip()\n",
    "    return string\n",
    "\n",
    "def appendHespressUrl(url):\n",
    "    return \"https://www.hespress.com\"+url\n",
    "\n",
    "#takes a news page and a topic and extracts the stories and comments\n",
    "def processPage(url,topic):\n",
    "    soup=toSoup(url)\n",
    "    title=soup.find(\"h1\",{\"class\":\"page_title\"}).text.strip()\n",
    "    date=soup.find(\"span\",{\"class\":\"story_date\"}).text.strip()\n",
    "    body=soup.find(\"div\",{\"id\":\"article_body\"})\n",
    "    author=body.find(\"span\",{\"class\":\"story_author\"}).text.strip()\n",
    "    story=paragraphArrayToSingleString(body.findAll(\"p\"))\n",
    "    postId=uuid.uuid1().hex\n",
    "    storiesArray.append({\"id\":postId,\"title\":title,\"date\":date,\"author\":author,\"story\":story,\"topic\":topic})\n",
    "    comments=soup.findAll(\"div\", {\"class\": \"comment_text\"})\n",
    "    results=soup.findAll(\"div\", {\"class\": \"result\"}) \n",
    "    for comment,result in zip(comments,results):\n",
    "        commentsArray.append({\"postId\":postId,\"comment\":comment.text.strip(),\"score\":result.text.strip(),\"topic\":topic})\n",
    "\n",
    "#Scrapes a topic for news urls\n",
    "def getPageUrls(topic,number):\n",
    "    urls=[]\n",
    "    soup=toSoup(\"https://www.hespress.com/\"+topic+\"/index.\"+number+\".html\")\n",
    "    firstOne=soup.find(\"h1\",{\"class\":\"section_title\"})\n",
    "    urls.append(appendHespressUrl(firstOne.find(\"a\")[\"href\"]))\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"short\"})\n",
    "    for div in divs:\n",
    "        urls.append(appendHespressUrl(div.find(\"a\")[\"href\"]))\n",
    "    return urls\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.1\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.2\n",
      "0.22\n",
      "0.24\n",
      "0.26\n",
      "0.28\n",
      "0.3\n",
      "0.32\n",
      "0.34\n",
      "0.36\n",
      "0.38\n",
      "0.4\n",
      "0.42\n",
      "0.44\n",
      "0.46\n",
      "0.48\n",
      "0.5\n",
      "0.52\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.6\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.7\n",
      "0.72\n",
      "0.74\n",
      "0.76\n",
      "0.78\n",
      "0.8\n",
      "0.82\n",
      "0.84\n",
      "0.86\n",
      "0.88\n",
      "0.9\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "1.0\n",
      "art-et-culture is done ! \n",
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.1\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.2\n",
      "0.22\n",
      "0.24\n",
      "0.26\n",
      "0.28\n",
      "0.3\n",
      "0.32\n",
      "0.34\n",
      "0.36\n",
      "0.38\n",
      "0.4\n",
      "0.42\n",
      "0.44\n",
      "0.46\n",
      "0.48\n",
      "0.5\n",
      "0.52\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.6\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.7\n",
      "0.72\n",
      "0.74\n",
      "0.76\n",
      "0.78\n",
      "0.8\n",
      "0.82\n",
      "0.84\n",
      "0.86\n",
      "0.88\n",
      "0.9\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "1.0\n",
      "tamazight is done ! \n",
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.1\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.2\n",
      "0.22\n",
      "0.24\n",
      "0.26\n",
      "0.28\n",
      "0.3\n",
      "0.32\n",
      "0.34\n",
      "0.36\n",
      "0.38\n",
      "0.4\n",
      "0.42\n",
      "0.44\n",
      "0.46\n",
      "0.48\n",
      "0.5\n",
      "0.52\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.6\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.7\n",
      "0.72\n",
      "0.74\n",
      "0.76\n",
      "0.78\n",
      "0.8\n",
      "0.82\n",
      "0.84\n",
      "0.86\n",
      "0.88\n",
      "0.9\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "1.0\n",
      "orbites is done ! \n",
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.1\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.2\n",
      "0.22\n",
      "0.24\n",
      "0.26\n",
      "0.28\n",
      "0.3\n",
      "0.32\n",
      "0.34\n",
      "0.36\n",
      "0.38\n",
      "0.4\n",
      "0.42\n",
      "0.44\n",
      "0.46\n",
      "0.48\n",
      "0.5\n",
      "0.52\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.6\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.7\n",
      "0.72\n",
      "0.74\n",
      "0.76\n",
      "0.78\n",
      "0.8\n",
      "0.82\n",
      "0.84\n",
      "0.86\n",
      "0.88\n",
      "0.9\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "1.0\n",
      "sport is done ! \n"
     ]
    }
   ],
   "source": [
    "#N is the number of pages to scrape per topic each page is 20 news articles so 50 pages gives 50*20=1000 news article per topic\n",
    "\n",
    "N=50\n",
    "topics=[\"politique\",\"regions\",\"societe\",\"economie\",\"marocains-du-monde\",\"medias\",\"faits-divers\",\"art-et-culture\",\"tamazight\",\"orbites\",\"sport\"]\n",
    "for topic in topics:\n",
    "    commentsArray=[]\n",
    "    storiesArray=[]\n",
    "    for i in range(50):\n",
    "        urls=getPageUrls(topic,str(i+1))\n",
    "        for url in urls:\n",
    "            processPage(url,topic)\n",
    "        print((i+1)/50)\n",
    "    pd.DataFrame(commentsArray).to_csv(\"comments_\"+topic+\".csv\")\n",
    "    pd.DataFrame(storiesArray).to_csv(\"stories_\"+topic+\".csv\")\n",
    "    print(topic+\" is done ! \")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
